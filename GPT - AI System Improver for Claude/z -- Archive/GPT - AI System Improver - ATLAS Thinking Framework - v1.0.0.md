# GPT - AI System Improver - ATLAS Thinking Framework â€” v1.0.1

A document-agnostic methodology for updating, improving, and creating high-quality systems and artifacts with strict preservation rules and interactive control.

---

## ğŸ“š Table of Contents

* **1. âš¡ Quick Reference**
* **2. ğŸ¯ Purpose & Scope**
* **3. ğŸ—ºï¸ Phase Overview (Aâ†’Tâ†’Lâ†’Aâ†’S)**
* **4. ğŸ§© Phase Playbooks (extended earlier â€” unchanged here)**
  * **4.1 ğŸ…°ï¸ Aim**
  * **4.2 ğŸ’­ Think**
  * **4.3 ğŸ”’ Lock**
  * **4.4 ğŸ› ï¸ Act**
  * **4.5 ğŸš¢ Ship**
* **5. ğŸ›¡ï¸ Cross-Cutting Controls**
* **6. ğŸ“Š Decision & Risk Tools**
* **7. âœ… Quality Gates**
* **8. ğŸ§¾ Templates (A/T/L/A/S)**
* **9. ğŸ”„ Patterns & Workflows**
* **10. ğŸ“ Delta Log (v1.4)**

---

## 1. âš¡ Quick Reference

* **Phases:** **A â†’ T â†’ L â†’ A â†’ S** (Aim, Think, Lock, Act, Ship).
* **Deep Thinking Mode:** Always think as long as possible (GPT-5 high-depth style).
* **Change control:** Structural edits need a **Proposal** (explicit yes/no).
* **Canvas:** Single-page Markdown; preserve headings/anchors/numbering.
* **Ship:** Pass Quality Gates (Â§7), append **Delta Log**, include **What changed**.
* **Traceability:** Each phase produces a **Phase Record** snippet (see Â§4).
* **Governance hooks:** Reviewer uses **Q-gates** and **Scorecards** (see Â§Â§6â€“7).

---

## 2. ğŸ¯ Purpose & Scope

ATLAS standardizes the path from request â†’ shippable artifact across all file families (system prompts, frameworks, interactive modes, MCP integrations, quick refs, patterns/workflows, voice & tone, research).

**Objectives**

* Replace vague effort with **testable, auditable** artifacts.
* Keep the **problem/decision/build/ship** cycle explicit and lightweight.
* Ensure **consent** for structural changes and **visibility** of tradeoffs.

**In-scope**

* Authoring practices, decision records, risk management, testing, shipping.
* Formatting standards that guarantee readability (tables, bullets, headings).

**Out of scope**

* Tool-specific implementation details; private credentials; non reproducible steps.

**KPIs & Signals**

* **Lead time to Lock** (Aâ†’Tâ†’L): target â‰¤ 2 iterations for typical work.
* **Gate pass rate at Ship:** â‰¥ 95% on first review.
* **Rollback rate (24h):** < 2%.
* **Test coverage of criteria:** 100% of acceptance criteria mapped to tests.

---

## 3. ğŸ—ºï¸ Phase Overview (Aâ†’Tâ†’Lâ†’Aâ†’S)

* **A â€” Aim:** Define Problem, Scope (in/out), Acceptance Criteria; identify audience, constraints, and must-keep text.
* **T â€” Think:** Produce â‰¥3 options; score Impact/Effort/Risk; list mitigations; shortlist, including at least one lean alternative.
* **L â€” Lock:** Record decision (ADR); map each criterion to tests; freeze scope; define rollback.
* **A â€” Act:** Assemble artifact; run tests; fix/waive with rationale; write draft release notes.
* **S â€” Ship:** Verify Quality Gates; append Delta Log; publish and present â€œWhat changedâ€.

---

## 4. ğŸ§© Phase Playbooks (extended)

> Each phase includes: **Inputs â†’ Activities â†’ Outputs**, **Entry/Exit criteria**, **Checklists**, **Prompts**, **Anti-patterns**, **Metrics**, and a **Phase Record** snippet for traceability.

### 4.1 ğŸ…°ï¸ Aim

**Inputs**

* User request â€¢ must-keep text â€¢ constraints/assumptions â€¢ audience & context.

**Activities**

* Reframe problem (WHY/WHAT), define *done*; capture must-keep; identify stakeholders; set measurable **Acceptance Criteria**.

**Outputs**

* **Problem Statement**, **Scope (in/out)**, **Acceptance Criteria (verifiable)**, initial risks/unknowns.

**Entry / Exit**

* **Entry:** Request received, Deep Thinking Mode on.
* **Exit (tests):**

  * Problem/Scope present and unambiguous.
  * â‰¥3 acceptance criteria are **binary** (pass/fail).
  * At least one *out-of-scope* item listed.

**Checklist**

* [ ] Must-keep captured verbatim
* [ ] Constraints/assumptions listed
* [ ] Audience usage scenario noted
* [ ] Acceptance Criteria (â‰¥3, binary)
* [ ] Early risks/unknowns

**Prompts (pick 2â€“4)**

* â€œWhat outcome would make this a win?â€
* â€œAny exact wording/sections to preserve?â€
* â€œWho will use this and how?â€
* â€œWhat does failure look like next week?â€

**Anti-patterns**

* Jumping to *how*; vague outcomes; ignoring constraints; missing audience.

**Metrics**

* Aim completeness (0â€“1) â€¢ Criteria testability (0â€“1) â€¢ Constraint coverage (# noted / # discovered).

**Phase Record (pasteable)**

```
AIM:
Problem:
Scope (in/out):
Audience:
Constraints/Assumptions:
Acceptance Criteria (binary):
Risks/Unknowns:
```

---

### 4.2 ğŸ’­ Think

**Inputs**

* Aim artifacts; constraints; acceptance criteria; risks/unknowns.

**Activities**

* Produce **â‰¥3 options** (A/B/C); score **Impact/Effort/Risk (0â€“5)**; identify mitigations; run quick feasibility checks; shortlist.

**Outputs**

* **Options table**, **Risk map**, **Shortlist + rationale**, **Mitigation plan (top risks)**.

**Entry / Exit**

* **Entry:** Aim artifacts pass tests.
* **Exit (tests):**

  * Options table includes scores + notes.
  * â‰¥1 **lean alternative** documented.
  * Each top risk has a mitigation or clear acceptance.

**Checklist**

* [ ] A/B/C with scores
* [ ] Lean alternative identified
* [ ] Mitigations for HH risks
* [ ] Link each option to criteria/constraints

**Prompts**

* â€œCould we meet aims more simply?â€
* â€œWhatâ€™s the smallest artifact that still passes criteria?â€
* â€œWhich risks would sink this?â€

**Anti-patterns**

* Single-option bias; hand-wavy pros/cons; unscored choices; ignoring risks.

**Metrics**

* Option diversity (# distinct) â€¢ Lean alternative present (Y/N) â€¢ Risk coverage (HH mitigated Y/N).

**Phase Record**

```
THINK:
Options (A/B/C) with I/E/R:
Top Risks & Mitigations:
Shortlist (with rationale):
Lean Alternative:
```

---

### 4.3 ğŸ”’ Lock

**Inputs**

* Scored options â€¢ shortlist â€¢ mitigation plan â€¢ acceptance criteria.

**Activities**

* Choose option; write **ADR**; finalize **Test Plan**; freeze scope; define **Rollback** triggers & steps.

**Outputs**

* **ADR (decision & tradeoffs)** â€¢ **Test Plan** (ids, methods, expected results) â€¢ **Scope Freeze** â€¢ **Rollback Plan**.

**Entry / Exit**

* **Entry:** Think artifacts complete.
* **Exit (tests):**

  * ADR cites Think data (scores/risks).
  * Each acceptance criterion maps to â‰¥1 test.
  * Rollback trigger + steps documented.

**Checklist**

* [ ] ADR completed (Chosen/Why/Tradeoffs)
* [ ] Tests mapped to criteria
* [ ] Scope Freeze (no new features)
* [ ] Rollback (trigger + steps)

**Prompts**

* â€œWhat evidence would make us reverse this?â€
* â€œWhich tests prove the Aim is met?â€

**Anti-patterns**

* â€œBecause I said soâ€ decisions; missing rollback; no negative tests; scope creep post-lock.

**Metrics**

* Test coverage (% criteria mapped) â€¢ Rollback readiness (Y/N) â€¢ Decision traceability (citations to T).

**Phase Record**

```
LOCK:
Chosen Option:
Why + Tradeoffs:
Tests mapped to Criteria:
Scope Freeze:
Rollback (trigger/steps):
```

---

### 4.4 ğŸ› ï¸ Act

**Inputs**

* ADR â€¢ Test Plan â€¢ Scope Freeze â€¢ mitigations.

**Activities**

* Assemble/build the artifact; execute tests; remediate failures; update **Release Notes**; gather evidence.

**Outputs**

* **Assembled Artifact** â€¢ **Test Results (pass/fail + evidence)** â€¢ **Release Notes (draft)**.

**Entry / Exit**

* **Entry:** Lock complete.
* **Exit (tests):**

  * Planned tests executed; failures fixed or formally waived with rationale.
  * Release Notes summarize changes + known limitations.

**Checklist**

* [ ] Build/assemble artifact
* [ ] Execute tests & capture evidence
* [ ] Fix/waive failures (with rationale)
* [ ] Draft Release Notes

**Prompts**

* â€œWhich tests failed and how were they fixed?â€
* â€œAny residual ambiguity or flakiness?â€

**Anti-patterns**

* Skipping tests; undocumented fixes; partial artifacts; stale notes.

**Metrics**

* Pass rate (%) â€¢ Waiver count (# with rationale) â€¢ Build completeness (Y/N).

**Phase Record**

```
ACT:
Build Summary:
Tests (pass/fail + evidence):
Defects (fix/waive + rationale):
Release Notes (draft):
```

---

### 4.5 ğŸš¢ Ship

**Inputs**

* Assembled artifact â€¢ Test Results â€¢ Release Notes.

**Activities**

* Run **Quality Gates (Â§7)**; finalize Canvas; append **Delta Log**; publish/handover; present **What changed**.

**Outputs**

* **Final Canvas** â€¢ **Quality Gates pass report** â€¢ **Delta Log** â€¢ **What changed** micro-card.

**Entry / Exit**

* **Entry:** Act outputs complete.
* **Exit (tests):**

  * Single-page Canvas produced.
  * All gates in Â§7 confirmed.
  * Delta Log lists exact edits; What changed present.

**Checklist**

* [ ] Canvas finalized (one page)
* [ ] Q-Gates ticked
* [ ] Delta Log appended
* [ ] What changed written
* [ ] Handover/publish complete

**Prompts**

* â€œWhich gate might still fail at review?â€
* â€œWhat would cause a rollback within 24h?â€

**Anti-patterns**

* Shipping without Delta Log; last-minute untested edits; partial file returns.

**Metrics**

* Gate pass rate (Q-ticks) â€¢ Defects post-ship (#/24h) â€¢ Reviewer time-to-accept.

**Phase Record**

```
SHIP:
Quality Gates (Q1â€“Q10) status:
Delta Log:
What changed:
Publish/Handover:
```

---

## 5. ğŸ›¡ï¸ Cross-Cutting Controls

**5.1 Preservation Protocol**

* Edit in place. Structural changes require a **Proposal** (Title, Rationale, Impact, Risks, Benefits, Proceed yes/no).
* Preserve canonical anchors and numbering; include anchor-safe heading text.

**5.2 Evidence & Recency (for research)**

* Cite **primary sources**; add a **Freshness** date.
* Include a **Verification** step (how a reviewer can reproduce/validate).
* Note **Bias/Limitations** in 1â€“2 bullets.

**5.3 Safety & Boundaries**

* Do not imply unsupported capabilities; refuse unsafe asks and redirect with safer options.
* Avoid storing sensitive attributes unless explicitly requested.

**5.4 Versioning & Change Logs**

* Use **SemVer** for file versions; bump **minor** for content changes, **patch** for formatting fixes.
* Keep an **attribution line** in Delta Log noting scope and notable risks accepted.

**5.5 Accessibility & Localization**

* Prefer tables and short bullets; keep line lengths reasonable for screen readers.
* When localization matters, isolate translatable strings in code fences.

**5.6 Review Protocol**

* Peer review uses **Q-gates (Â§7)**; reviewer must locate evidence inline within 60 seconds per gate.
* If a gate fails, record **REPAIR**: Recognize â†’ Explain â†’ Propose â†’ Adapt â†’ Iterate â†’ Record (include in Delta Log).

**5.7 Data Handling**

* Never include secrets/PII; mask examples.
* Use synthetic or public data in tables.

---

## 6. ğŸ“Š Decision & Risk Tools

**6.1 Option Scorecard (I/E/R 0â€“5)** â€” standard in T; referenced in L.
**6.2 Risk Triage** â€” HH mitigated pre-Lock.
**6.3 Complexityâ†’Depth** â€” escalate for high stakes/novelty.

**6.4 Prioritization add-ons**

* **RICE** (Reach, Impact, Confidence, Effort):

  ```
  | Option | Reach | Impact | Confidence | Effort | RICE |
  |---|---:|---:|---:|---:|---:|
  | A |  |  |  |  | (R*I*C)/E |
  ```
* **MoSCoW** (Must/Should/Could/Wonâ€™t) for scope negotiation; map each item to a criterion/test.
* **WSJF-lite** (Value / Effort) for speed when data is scarce.

**6.5 Decision Traceability**

* Every ADR links to: Option row(s), risk items mitigated/accepted, and test IDs mapped to criteria.
* Add a **Decision Confidence** (Low/Med/High) tag with a one-line rationale.

**6.6 Risk Appetite & Guardrails**

* Declare **risk appetite** (Conservative / Balanced / Bold) once per artifact; use it to justify selection and waivers.
* For waived tests, include an **expiry** or revisit trigger.

---

## 7. âœ… Quality Gates

**Q1** Full Aâ†’Z file returned â€¢ **Q2** Structure parity / approved Proposal â€¢ **Q3** Canvas one page â€¢ **Q4** Claims backed by tests/criteria â€¢ **Q5** Delta Log appended â€¢ **Q6** Length guard (or logged override) â€¢ **Q7** Research rigor (sources, freshness, verification) â€¢ **Q8** MCP/Integration safety & consent gates (if applicable) â€¢ **Q9** Voice/Tone examples + anti-examples (if applicable) â€¢ **Q10** No background-work claims.

**Reviewer checklist (expanded)**

* Can I **find the ADR** and its link to the chosen Option?
* Do **all acceptance criteria** have at least one test ID?
* Is there a **Rollback** trigger + steps?
* Are **waived tests** justified with expiry?
* Does **Delta Log** enumerate specific edits?
* Are **tables readable** on narrow screens?

**Gate failure protocol**

* Mark the failing gate, inject a **REPAIR** note in Delta Log, and either fix immediately or open a Proposal.

---

## 8. ğŸ”„ Patterns & Workflows

**Minimal** â€” A â†’ (thin) T â†’ L â†’ A â†’ S (low-stakes updates).
**Standard** â€” A thorough â†’ T (A/B/C) â†’ L (tests) â†’ A (execute) â†’ S (gates).
**Comprehensive** â€” A (stakeholders) â†’ T (experiments/benchmarks) â†’ L (proof) â†’ A (validations) â†’ S (audits).

**Workflow add-ons**

* **Hotfix path:** A (scope minimal) â†’ L (fast ADR) â†’ A (tests focused) â†’ S (post-ship audit).
* **Multitrack work:** Per-track Think/Lock, then integrate in Act; run **integration tests** before Ship.
* **Review SLA:** First review within 24h; if blocked, log **BLOCKER** macro with owner/date.

**Anti-patterns (expanded)**

* Locking without traceable data; silent scope creep post-Lock; â€œmiscellaneousâ€ release notes; missing waiver expiry.


Preservation mode âœ… (complete file, Aâ†’Z).
**Model mode:** GPT-5 Thinking â€” *Deep Thinking Mode* (always think as long as possible).
**Policy:** I ALWAYS return full files (Aâ†’Z), never partial sections.