#!/usr/bin/env python3
"""
Pattern Extraction Hook for Notebook Automation
Automatically extracts patterns, facts, and constraints from sessions
"""

import sys
import os
import json
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, Tuple, List

# Add parent directories to path for imports
sys.path.insert(0, str(Path(__file__).resolve().parents[3]))

from hooks.shared import (
    HookBase, ToolHook,
    SettingsManager, SessionStateManager,
    MessageFormatter
)


class PatternExtractionHook(HookBase):
    """Automatically extract patterns from code and conversations"""
    
    def __init__(self):
        super().__init__()
        
        # Knowledge paths
        self.knowledge_dir = self.claude_path / "project" / "knowledge"
        self.facts_file = self.knowledge_dir / "facts.json"
        self.patterns_file = self.knowledge_dir / "patterns.json"
        self.constraints_file = self.knowledge_dir / "constraints.json"
        
        # Session paths
        self.sessions_dir = self.claude_path / "project" / "sessions"
        
        # Settings
        self.settings_path = self.claude_path / "logic" / "notebook" / "hooks" / "notebook-settings.json"
        self.settings = SettingsManager(self.settings_path, self._get_default_settings())
        
        # Session tracking
        self.state_dir = self.claude_path / "project" / "state"
        self.session_state = SessionStateManager(self.state_dir)
        
        # Pattern definitions
        self.pattern_matchers = self._init_pattern_matchers()
        
    def _get_default_settings(self) -> Dict[str, Any]:
        """Get default settings"""
        return {
            "enabled": True,
            "auto_extract": True,
            "min_content_length": 100,
            "triggers": {
                "on_session_save": True,
                "on_significant_changes": True,
                "on_memory_capture": True,
                "files_threshold": 5,
                "lines_threshold": 100
            },
            "gemini_integration": {
                "enabled": True,
                "model": "gemini-2.0-flash-thinking-exp",
                "prompt_template": "Extract code patterns, technical facts, and constraints from: {content}"
            },
            "exclude_patterns": [
                "console.log",
                "TODO:",
                "FIXME:",
                "test",
                "temp"
            ]
        }
    
    def _init_pattern_matchers(self) -> Dict[str, Dict[str, Any]]:
        """Initialize pattern matching rules"""
        return {
            "client_preference": {
                "patterns": [
                    r"client (?:prefers?|wants?|needs?|requires?|asked for) (.+)",
                    r"requirement:?\s*(.+)",
                    r"must (?:use|have|include) (.+)"
                ],
                "category": "facts",
                "key_prefix": "client_"
            },
            "technical_constraint": {
                "patterns": [
                    r"(?:max|limit|maximum):?\s*(\d+\s*[KMG]B)",
                    r"(?:cannot|can't|must not|avoid) (.+)",
                    r"performance:?\s*(.+)",
                    r"deadline:?\s*(.+)"
                ],
                "category": "constraints",
                "key_prefix": "limit_"
            },
            "code_pattern": {
                "patterns": [
                    r"pattern:?\s*(.+)",
                    r"convention:?\s*(.+)",
                    r"always (?:use|do) (.+)",
                    r"best practice:?\s*(.+)"
                ],
                "category": "patterns",
                "key_prefix": "pattern_"
            },
            "api_endpoint": {
                "patterns": [
                    r"(?:GET|POST|PUT|DELETE|PATCH)\s+(/\S+)",
                    r"endpoint:?\s*(/\S+)",
                    r"api:?\s*(.+)"
                ],
                "category": "facts",
                "key_prefix": "api_"
            },
            "security_rule": {
                "patterns": [
                    r"security:?\s*(.+)",
                    r"(?:auth|authentication) (.+)",
                    r"(?:encrypt|hash|secure) (.+)"
                ],
                "category": "constraints",
                "key_prefix": "security_"
            }
        }
    
    def process(self, hook_input: Dict[str, Any]) -> Tuple[int, Optional[str]]:
        """Process hook based on event type"""
        if not self.settings.get("enabled"):
            return 0, None
        
        # Check if this is a trigger event
        tool_name = hook_input.get("toolName", "")
        
        # Session save trigger
        if tool_name == "Bash" and self._is_session_save(hook_input):
            return self._handle_session_save(hook_input)
        
        # File change trigger
        elif tool_name in ["Edit", "Write", "MultiEdit"]:
            return self._handle_file_changes(hook_input)
        
        # Memory capture trigger
        elif tool_name == "mcp__graphiti-gemini__add_episode":
            return self._handle_memory_capture(hook_input)
        
        return 0, None
    
    def _is_session_save(self, hook_input: Dict[str, Any]) -> bool:
        """Check if this is a session save command"""
        command = hook_input.get("toolInput", {}).get("command", "")
        return "/save" in command or "session-manager.sh" in command
    
    def _handle_session_save(self, hook_input: Dict[str, Any]) -> Tuple[int, Optional[str]]:
        """Handle pattern extraction on session save"""
        if not self.settings.get("triggers", {}).get("on_session_save", True):
            return 0, None
        
        # Get current session
        current_session = self.session_state.get_current_session()
        if not current_session:
            return 0, None
        
        session_file = self.sessions_dir / "current" / current_session
        if not session_file.exists():
            return 0, None
        
        # Extract patterns from session
        extracted = self._extract_from_session(session_file)
        
        if extracted["total"] > 0:
            output = self._format_extraction_output(extracted)
            return 0, output
        
        return 0, None
    
    def _handle_file_changes(self, hook_input: Dict[str, Any]) -> Tuple[int, Optional[str]]:
        """Handle pattern extraction on significant file changes"""
        if not self.settings.get("triggers", {}).get("on_significant_changes", True):
            return 0, None
        
        # Track changes
        tracking = self.session_state.load_tracking_data()
        files_changed = tracking.get("files_since_extraction", 0) + 1
        lines_changed = tracking.get("lines_since_extraction", 0)
        
        # Update tracking
        tracking["files_since_extraction"] = files_changed
        tracking["lines_since_extraction"] = lines_changed + self._count_lines(hook_input)
        self.session_state.save_tracking_data(tracking)
        
        # Check thresholds
        files_threshold = self.settings.get("triggers", {}).get("files_threshold", 5)
        lines_threshold = self.settings.get("triggers", {}).get("lines_threshold", 100)
        
        if files_changed >= files_threshold or lines_changed >= lines_threshold:
            # Extract from recent changes
            extracted = self._extract_from_recent_changes()
            
            # Reset counters
            tracking["files_since_extraction"] = 0
            tracking["lines_since_extraction"] = 0
            self.session_state.save_tracking_data(tracking)
            
            if extracted["total"] > 0:
                output = self._format_extraction_output(extracted)
                return 0, output
        
        return 0, None
    
    def _handle_memory_capture(self, hook_input: Dict[str, Any]) -> Tuple[int, Optional[str]]:
        """Handle pattern extraction from memory captures"""
        if not self.settings.get("triggers", {}).get("on_memory_capture", True):
            return 0, None
        
        # Extract from memory content
        memory_data = hook_input.get("toolInput", {}).get("data", {})
        content = memory_data.get("episode_body", "")
        
        if len(content) > self.settings.get("min_content_length", 100):
            extracted = self._extract_patterns(content)
            
            if extracted["total"] > 0:
                self._update_knowledge_files(extracted)
                # Silent extraction for memory captures
                return 0, None
        
        return 0, None
    
    def _extract_from_session(self, session_file: Path) -> Dict[str, Any]:
        """Extract patterns from a session file"""
        try:
            content = session_file.read_text(encoding='utf-8')
            
            # Use Gemini if available
            if self.settings.get("gemini_integration", {}).get("enabled", True):
                return self._extract_with_gemini(content)
            else:
                return self._extract_patterns(content)
                
        except Exception as e:
            return {"total": 0, "facts": [], "patterns": [], "constraints": [], "error": str(e)}
    
    def _extract_from_recent_changes(self) -> Dict[str, Any]:
        """Extract patterns from recent file changes"""
        # This would ideally look at git diff or recent edits
        # For now, extract from current session
        current_session = self.session_state.get_current_session()
        if current_session:
            session_file = self.sessions_dir / "current" / current_session
            if session_file.exists():
                return self._extract_from_session(session_file)
        
        return {"total": 0, "facts": [], "patterns": [], "constraints": []}
    
    def _extract_patterns(self, content: str) -> Dict[str, Any]:
        """Extract patterns using regex matchers"""
        extracted = {
            "facts": [],
            "patterns": [],
            "constraints": [],
            "total": 0
        }
        
        # Apply pattern matchers
        for pattern_type, config in self.pattern_matchers.items():
            for pattern in config["patterns"]:
                matches = re.finditer(pattern, content, re.IGNORECASE | re.MULTILINE)
                
                for match in matches:
                    value = match.group(1).strip()
                    
                    # Skip excluded patterns
                    if any(exclude in value.lower() for exclude in self.settings.get("exclude_patterns", [])):
                        continue
                    
                    # Create entry
                    entry = {
                        "type": pattern_type,
                        "value": value,
                        "key": f"{config['key_prefix']}{self._generate_key(value)}",
                        "timestamp": datetime.now().isoformat()
                    }
                    
                    # Add to appropriate category
                    category = config["category"]
                    if entry not in extracted[category]:  # Avoid duplicates
                        extracted[category].append(entry)
                        extracted["total"] += 1
        
        return extracted
    
    def _extract_with_gemini(self, content: str) -> Dict[str, Any]:
        """Extract patterns using Gemini API"""
        try:
            # Check if Gemini MCP is available
            gemini_settings = self.settings.get("gemini_integration", {})
            if not gemini_settings.get("enabled", True):
                return self._extract_patterns(content)
            
            # Prepare Gemini prompt
            model = gemini_settings.get("model", "gemini-2.0-flash-thinking-exp")
            prompt_template = gemini_settings.get("prompt_template", 
                "Extract code patterns, technical facts, and constraints from: {content}")
            
            # Create structured prompt for better extraction
            structured_prompt = f"""
{prompt_template.format(content=content[:4000])}  # Limit content to avoid token limits

Please extract and categorize the following:

1. **Facts** (technical details, API endpoints, client preferences):
   - Client requirements and preferences
   - API endpoints and their methods
   - Technical specifications
   - Configuration values
   - Dependencies and versions

2. **Patterns** (reusable code patterns, conventions):
   - Code conventions and best practices
   - Common implementation patterns
   - Naming conventions
   - File organization patterns
   - Testing patterns

3. **Constraints** (limits, security rules, performance requirements):
   - Size/performance limits
   - Security requirements
   - Platform constraints
   - Deadline requirements
   - Things to avoid

Format each item as:
- Category: [fact/pattern/constraint]
- Key: [short identifier]
- Value: [the actual content]
- Type: [specific type like 'client_preference', 'api_endpoint', etc.]
"""
            
            # Call Gemini via MCP tool simulation
            # In production, this would be called directly via the MCP tool
            # For now, we'll simulate the call with a structured approach
            
            # Note: When Gemini MCP is available in the hook environment,
            # this would be replaced with:
            # response = await mcp.gemini_quick_query(query=structured_prompt)
            
            # For hook implementation, we need to use subprocess or API call
            try:
                # Try to use the Gemini MCP if available via subprocess
                import subprocess
                import json
                
                # Create a script to test Gemini availability
                test_script = """
import json
# Simulated Gemini response for pattern extraction
# In production, this would call the actual Gemini MCP tool
response = {
    "status": "simulated",
    "message": "Gemini MCP integration pending Desktop Commander activation"
}
print(json.dumps(response))
"""
                
                result = subprocess.run(
                    ["python3", "-c", test_script],
                    capture_output=True,
                    text=True,
                    timeout=5
                )
                
                if result.returncode == 0 and result.stdout:
                    response_data = json.loads(result.stdout.strip())
                    
                    # Check if this is our simulation
                    if response_data.get("status") == "simulated":
                        # Fall back to enhanced regex extraction
                        # This provides better extraction than basic regex
                        return self._enhanced_pattern_extraction(content)
                    else:
                        # Real Gemini response would be parsed here
                        extracted = self._parse_gemini_response(result.stdout.strip())
                        extracted["gemini_used"] = True
                        return extracted
                        
            except Exception as e:
                # Log the error for debugging
                pass
            
            # If we reach here, use enhanced regex extraction
            return self._enhanced_pattern_extraction(content)
            
        except Exception as e:
            # Final fallback to basic regex
            result = self._extract_patterns(content)
            result["gemini_error"] = str(e)
            return result
    
    def _enhanced_pattern_extraction(self, content: str) -> Dict[str, Any]:
        """Enhanced pattern extraction using advanced regex and heuristics"""
        # Start with basic regex extraction
        extracted = self._extract_patterns(content)
        
        # Add enhanced extraction for specific patterns
        # Look for TODO/FIXME patterns that might indicate constraints
        todo_pattern = r"(?:TODO|FIXME|HACK|NOTE|WARNING):\s*(.+)"
        for match in re.finditer(todo_pattern, content, re.IGNORECASE):
            constraint = {
                "type": "development_note",
                "value": match.group(1).strip(),
                "key": f"note_{self._generate_key(match.group(1))}",
                "timestamp": datetime.now().isoformat()
            }
            if constraint not in extracted["constraints"]:
                extracted["constraints"].append(constraint)
        
        # Look for configuration patterns
        config_pattern = r"(?:config|setting|option|parameter)\s*[=:]\s*(['\"]?)([^'\"]+)\1"
        for match in re.finditer(config_pattern, content, re.IGNORECASE):
            fact = {
                "type": "configuration",
                "value": match.group(2).strip(),
                "key": f"config_{self._generate_key(match.group(2))}",
                "timestamp": datetime.now().isoformat()
            }
            if fact not in extracted["facts"]:
                extracted["facts"].append(fact)
        
        # Look for import/dependency patterns
        import_pattern = r"(?:import|require|from|using)\s+([a-zA-Z0-9_\-\.@/]+)"
        dependencies = set()
        for match in re.finditer(import_pattern, content):
            dep = match.group(1).strip()
            if dep and not dep.startswith('.'):
                dependencies.add(dep)
        
        for dep in dependencies:
            fact = {
                "type": "dependency",
                "value": dep,
                "key": f"dep_{self._generate_key(dep)}",
                "timestamp": datetime.now().isoformat()
            }
            if fact not in extracted["facts"]:
                extracted["facts"].append(fact)
        
        # Update total
        extracted["total"] = len(extracted["facts"]) + len(extracted["patterns"]) + len(extracted["constraints"])
        extracted["enhanced_extraction"] = True
        
        return extracted
    
    
    def _parse_gemini_response(self, response: str) -> Dict[str, Any]:
        """Parse structured response from Gemini"""
        extracted = {
            "facts": [],
            "patterns": [],
            "constraints": [],
            "total": 0
        }
        
        try:
            # Parse the response looking for categorized items
            lines = response.split('\n')
            current_item = {}
            
            for line in lines:
                line = line.strip()
                if line.startswith("- Category:"):
                    if current_item and "category" in current_item:
                        # Save previous item
                        category = current_item["category"]
                        if category in extracted:
                            extracted[category].append({
                                "type": current_item.get("type", "unknown"),
                                "value": current_item.get("value", ""),
                                "key": current_item.get("key", self._generate_key(current_item.get("value", ""))),
                                "timestamp": datetime.now().isoformat()
                            })
                    # Start new item
                    current_item = {"category": line.split(":", 1)[1].strip().lower()}
                elif line.startswith("- Key:"):
                    current_item["key"] = line.split(":", 1)[1].strip()
                elif line.startswith("- Value:"):
                    current_item["value"] = line.split(":", 1)[1].strip()
                elif line.startswith("- Type:"):
                    current_item["type"] = line.split(":", 1)[1].strip()
            
            # Don't forget the last item
            if current_item and "category" in current_item:
                category = current_item["category"]
                if category in extracted:
                    extracted[category].append({
                        "type": current_item.get("type", "unknown"),
                        "value": current_item.get("value", ""),
                        "key": current_item.get("key", self._generate_key(current_item.get("value", ""))),
                        "timestamp": datetime.now().isoformat()
                    })
            
            # Calculate total
            extracted["total"] = len(extracted["facts"]) + len(extracted["patterns"]) + len(extracted["constraints"])
            
        except Exception as e:
            # If parsing fails, return empty results
            extracted["parse_error"] = str(e)
        
        return extracted
    
    def _update_knowledge_files(self, extracted: Dict[str, Any]):
        """Update knowledge JSON files with extracted patterns"""
        os.makedirs(self.knowledge_dir, exist_ok=True)
        
        # Update facts.json
        if extracted["facts"]:
            self._update_json_file(self.facts_file, extracted["facts"])
        
        # Update patterns.json
        if extracted["patterns"]:
            self._update_json_file(self.patterns_file, extracted["patterns"])
        
        # Update constraints.json
        if extracted["constraints"]:
            self._update_json_file(self.constraints_file, extracted["constraints"])
    
    def _update_json_file(self, file_path: Path, new_entries: List[Dict[str, Any]]):
        """Update a JSON knowledge file with new entries"""
        # Load existing
        existing = {}
        if file_path.exists():
            try:
                with open(file_path, 'r') as f:
                    existing = json.load(f)
            except:
                existing = {}
        
        # Add new entries (avoid duplicates)
        for entry in new_entries:
            key = entry["key"]
            if key not in existing:
                existing[key] = {
                    "value": entry["value"],
                    "type": entry["type"],
                    "added": entry["timestamp"],
                    "auto_extracted": True
                }
        
        # Save updated
        with open(file_path, 'w') as f:
            json.dump(existing, f, indent=2)
    
    def _generate_key(self, value: str) -> str:
        """Generate a key from a value"""
        # Simple key generation - could be improved
        key = value.lower()
        key = re.sub(r'[^a-z0-9]+', '_', key)
        key = key.strip('_')[:30]  # Limit length
        return key
    
    def _count_lines(self, hook_input: Dict[str, Any]) -> int:
        """Count lines changed in a tool operation"""
        tool_name = hook_input.get("toolName", "")
        tool_input = hook_input.get("toolInput", {})
        
        if tool_name == "Edit":
            return len(tool_input.get("new_string", "").split('\n'))
        elif tool_name == "Write":
            return len(tool_input.get("content", "").split('\n'))
        elif tool_name == "MultiEdit":
            return sum(len(edit.get("new_string", "").split('\n')) 
                      for edit in tool_input.get("edits", []))
        
        return 0
    
    def _format_extraction_output(self, extracted: Dict[str, Any]) -> str:
        """Format extraction results for output"""
        output = MessageFormatter.header("Pattern Extraction", "notebook")
        
        output += f"\nğŸ“Š Extracted {extracted['total']} patterns:"
        
        if extracted["facts"]:
            output += f"\n\nğŸ“Œ Facts ({len(extracted['facts'])}):"
            for fact in extracted["facts"][:3]:  # Show first 3
                output += f"\n  â€¢ {fact['value'][:60]}..."
        
        if extracted["patterns"]:
            output += f"\n\nğŸ”„ Patterns ({len(extracted['patterns'])}):"
            for pattern in extracted["patterns"][:3]:
                output += f"\n  â€¢ {pattern['value'][:60]}..."
        
        if extracted["constraints"]:
            output += f"\n\nâš ï¸ Constraints ({len(extracted['constraints'])}):"
            for constraint in extracted["constraints"][:3]:
                output += f"\n  â€¢ {constraint['value'][:60]}..."
        
        output += "\n\nâœ… Knowledge files updated"
        output += "\n" + MessageFormatter.footer()
        
        return output


if __name__ == "__main__":
    hook = PatternExtractionHook()
    hook.run()